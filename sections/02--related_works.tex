\section{Related Work}
\label{sec:RelatedWork}

\subsection{Literature Selection Methodology}

We situate multi-armed bandits (MABs) as a family of uncertainty-aware sequential decision rules and use quantum entanglement routing as a stress test where stochastic noise, structured disruption, and resource constraints jointly shape performance. Our scope spans: (i) finite-time regret analyses in stochastic and adversarial regimes, (ii) contextual and neural representations for structured environments, (iii) hybrid constructions that combine mechanisms across regimes, and (iv) predictive (informed) bandits that incorporate explicit forecasts or learned dynamics.

\subsubsection{Search Strategy and Time Span (2002--2025)}

We queried arXiv, IEEE Xplore, and the ACM Digital Library over the 2002--2025 window using combinations of:
\emph{multi-armed bandits, contextual bandits, adversarial bandits, neural bandits, exploration--exploitation, predictive bandits, online forecasting, informed contextual bandits, quantum routing}.
We then performed backward/forward snowballing from canonical anchors (\eg UCB/EXP3 foundations; LinUCB and neural contextual bandits; recent bandit-based quantum-routing studies), prioritizing work that either:
(a) introduced a reusable mechanism (confidence bounds, posterior sampling, adversarial randomization, pursuit-style updates, forecasting/world models), or
(b) demonstrated cross-domain transfer under materially different constraints (\eg routing, communications, finance, healthcare).

The chosen time span captures the modern finite-time theory for stochastic and adversarial learning, the rise of contextual formulations for structured decision-making, and recent neural, hybrid, and predictive variants designed for complex, partially observed environments.

\subsubsection{Inclusion and Exclusion Criteria}
    \begin{description}[leftmargin=1.5em]
      \item \textit{Included:}
        \begin{enumerate}[leftmargin=1.8em]
          \item Canonical stochastic and adversarial MAB algorithms with regret guarantees~\cite{auer2002finite,thompson1933likelihood,auer2002nonstochastic}.
          \item Contextual neural bandits that scale action selection to structured/high-dimensional contexts~\cite{li2010contextual,zhou2020neuralucb,zhang2022neuralts}.
          \item Predictive/informed contextual bandits that integrate forecasting or learned dynamics~\cite{kar2024icmab,box2015time}.
          \item Hybrid methods that combine mechanisms across regimes~\cite{thathachar2011networks}.
          \item Cross-domain applications where the same mechanism is instantiated under different reward and constraint models.
        \end{enumerate}
    
      \item \textit{Excluded:}
        \begin{enumerate}[leftmargin=1.8em]
          \item Offline optimization/control without online learning under bandit feedback.
          \item Single-domain demonstrations without reusable algorithmic insight.
          \item Pure tuning studies without methodological novelty, clearly stated assumptions, or reproducibility artifacts.
        \end{enumerate}
    \end{description}


\subsection{Foundational Bandits and Regret Regimes}

Foundational results formalize the exploration--exploitation trade-off and provide regret guarantees that define the efficiency--robustness envelope. In stochastic i.i.d.\ settings, UCB-style optimism and Thompson-style posterior sampling achieve logarithmic-in-horizon regret under standard gap conditions~\cite{auer2002finite,thompson1933likelihood}. In adversarial settings, EXP3 attains sublinear regret without stochastic assumptions, trading some benign-regime efficiency for worst-case protection~\cite{auer2002nonstochastic}. These regimes motivate why quantum routing evaluations should explicitly separate \emph{natural noise} from \emph{coordinated disruption}: the learning objective and the appropriate safety guarantees depend on the feedback model.

\subsection{Contextual and Neural Bandits}

Contextual bandits condition decisions on observable state, enabling structured decision-making when arms are not exchangeable. LinUCB models rewards as linear in context and selects actions via a confidence bonus~\cite{li2010contextual}. NeuralUCB and NeuralTS generalize this principle by learning representations with deep networks while retaining uncertainty-aware action selection through confidence-style bounds or sampling in representation space~\cite{zhou2020neuralucb,zhang2022neuralts}. The shared abstraction is mechanism-level:
\emph{learn a value predictor, maintain an uncertainty estimate over that predictor, and act optimistically or probabilistically.}
In quantum routing, the natural context includes topology/hop structure, link-quality indicators, and any measurable signals of load, memory, or temporal drift.

\subsection{Adversarial and Hybrid Robustness}

Adversarial bandits prioritize worst-case guarantees (e.g., EXP3-style randomization, which can be essential under nonstationarity or strategic manipulation~\cite{auer2002nonstochastic}). Hybrid designs aim to combine robust exploration with structured exploitation---for example, layering pursuit-style updates over context-conditioned value estimation or embedding adversarial weighting within learned reward models~\cite{thathachar2011networks}. Within quantum-routing studies, adversarial-first formulations are often motivated by jamming or targeted disruption; however, comparisons across families are frequently confounded by mismatched assumptions about allocation, memory/replay semantics, and evaluation taxonomies. This motivates treating allocation policies and replay parameterization as first-class experimental factors when assessing robustness under mixed threats.

\subsection{Predictive and Informed Bandits}

Predictive (informed) contextual bandits augment the decision rule with a forecasting or world-model component so that learning can anticipate drift rather than purely react to it. iCMAB exemplifies this direction by coupling contextual decision rules with an explicit predictive model of future context dynamics~\cite{kar2024icmab}. In our setting, we instantiate an informed variant by warming up predictive context with a classical time-series forecaster (ARIMA)~\cite{box2015time}. The core idea is again mechanism-level:
\emph{forecasting can augment the context signal}, but it does not replace the need for robust exploration and allocation policies under strategic threats.

\subsection{Quantum Network Routing with Bandits}

Recent quantum-network work applies bandits (and related online learning) to path selection under stochastic decoherence and, in some cases, structured disruption~\cite{wehner2018quantum,huang2024quantum,wang2025learning,li2025multipath,liu2024qbgp}. 
Wang et al.~\cite{wang2025learning} focus on learning high-quality paths under stochastic dynamics, while Li et al.~\cite{li2025multipath} propose multipath inter-domain routing protocols for quantum networks with online path selection; Liu et al.~\cite{liu2024qbgp} similarly emphasizes online benchmarking signals to support routing-policy adaptation. 
Wang et al.~\cite{wang2024adaptive} formulate an adaptive, user-centric entanglement routing problem with long-term budget constraints and propose an online control algorithm for per-slot routing and qubit allocation. In contrast, our contribution is to introduce and benchmark multiple allocator/decision-rule algorithms (including hybrid pursuit--neural and informed iCMAB variants) under a shared threat taxonomy, while treating allocator policy and replay/capacity semantics as explicit experimental factors. We do not propose a new quantum-network routing protocol or a new budgeted-control formulation with analytical guarantees; rather, we provide a controlled robustness characterization that isolates which algorithm--allocator--capacity combinations remain stable when disruption is structured or adaptive.
Huang et al.~\cite{huang2024quantum} propose \emph{EXPNeuralUCB}, a group neural bandit that combines EXP3-style adversarial exploration with NeuralUCB-style nonlinear reward modeling for joint path selection and qubit allocation. We advance this line by introducing pursuit--neural hybrids (e.g., \texttt{CPursuitNeuralUCB}, \texttt{iCPursuitNeuralUCB}) and show that they achieve higher scenario-aggregated efficiency with stronger stability-floor behavior than EXPNeuralUCB in our evaluation suites. Further, while Huang et al.\ treat allocation as a fixed component, our framework explicitly varies allocator strategy and replay capacity, revealing that these factors can be as critical to robustness as the learning rule itself.

\paragraph{Closest-work contrast (LinkSelFiE).}
Liu et al.~\cite{10621263} propose \emph{LinkSelFiE}, which targets the \emph{link-level} problem of selecting a high-fidelity entanglement link and estimating its fidelity when link qualities are unknown \emph{a priori}. They cast link selection as a best-arm identification task and couple it with a benchmarking-driven estimation procedure to reduce quantum resource consumption while still identifying high-quality links with high confidence. In contrast, our study targets the \emph{end-to-end routing} problem: joint \emph{path selection and qubit allocation} over time under five threat regimes, quantified through a controlled cross-product evaluation across algorithms, allocators, and replay-capacity semantics. Our framework can incorporate link-level fidelity signals (including LinkSelFiE-style estimation outputs) into the routing reward model, but our primary contribution is to characterize robustness and deployment trade-offs at the routing layer under structured and adaptive disruption.

Beyond bandit-style path selection, learning-based route selection under noisy quantum-network conditions has also been explored~\cite{chaudhary2023quantum}, and RL-based adaptive routing has been proposed via deep Q-networks~\cite{jallowkhan2025adaptive}. Our work differentiates by introducing pursuit--neural allocator algorithms and stress-testing them under structured and adaptive threats in addition to stochastic noise. Complementary non-learning routing designs emphasize structural decomposition (e.g., QuARC adaptive clustering~\cite{clayton2024quarc} and hierarchical routing for scalability~\cite{cicconetti2024scalable}) or repeater/efficiency constraints~\cite{kumar2024routing}, while cost-vector approaches optimize multi-path routing decisions through explicit objective formulations~\cite{leone2021costvector}. In contrast, our contribution is a controlled evaluation methodology that isolates how decision-rule families interact with allocation policies, replay semantics, and capacity across a shared threat taxonomy, enabling direct attribution of robustness to the \emph{algorithm--allocator--capacity} triad rather than to a single routing primitive.
Across studies, experimental assumptions differ materially---especially in how qubits are allocated across paths, how memory/replay is parameterized relative to the horizon, and how threat processes are modeled---which complicates direct algorithm-to-algorithm comparison. 
Our benchmark addresses this by evaluating multiple algorithm classes under a shared threat taxonomy and by treating allocator policy and replay configuration as explicit experimental factors rather than fixed background choices, enabling direct attribution of robustness to the \emph{algorithm--allocator--capacity} triad.
% [C-003] Dan request ("directly compare against existing papers, e.g., \cite{10621263}") addressed in this subsection; detailed rationale + checklist recorded in `tracking/PAPER-CHANGES-TRACKER.md`.

\subsection{Toward a Modular, Universal Bandit Stack}

Across domains, the recurring meta-problem is stable: \emph{choose actions under uncertainty using an uncertainty-aware value estimate.}
What varies is the adapter layer: how context is defined, how rewards are observed, and what constraints must be respected (e.g., qubit budgets, routing thresholds, replay semantics). We operationalize this view as a modular stack:
\begin{enumerate}[label=(\roman*),leftmargin=2em]
  \item an \emph{allocator/decision rule},
  \item an optional \emph{forecasting layer} that augments context,
  \item a \emph{domain adapter} mapping quantum-routing signals to the shared allocator interface.
\end{enumerate}
General benchmarking and utility frameworks for quantum networks motivate this evaluation-first framing~\cite{coopmans2021benchmark,kozlowski2022utility}; our work complements them by making learning-specific factors (threat models, allocators, replay/capacity semantics) explicit experimental variables.
Best-of-both-worlds bandit theory motivates this mixed-regime view: algorithms such as Tsallis-INF achieve strong guarantees in both stochastic and adversarial settings without knowing the regime a priori~\cite{zimmert2019optimal}.
In our setting, we operationalize this idea empirically by stress-testing the same allocator stack across stochastic, structured, and adaptive threats.

Within this stack, our study provides a controlled comparison of classical, contextual/neural, adversarial, and informed variants under stochastic and adaptive disruption, clarifying when robustness is determined not only by the decision rule, but also by allocator choice and replay configuration.
